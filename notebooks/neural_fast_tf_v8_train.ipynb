{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import imageio\n",
    "import h5py\n",
    "import tables\n",
    "####################################################################################################################\n",
    "# the vgg code is a separate py file obtained from here: https://github.com/anishathalye/neural-style\n",
    "#download and place it in the same location as the python notebook folder\n",
    "import vgg \n",
    "####################################################################################################################\n",
    "from decimal import Decimal\n",
    "import pickle\n",
    "import gzip\n",
    "import time\n",
    "import functools\n",
    "from functools import reduce\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the path to the vgg weights\n",
    "#the vgg network can be downloaded from http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat\n",
    "vgg_path = \"../models/imagenet-vgg-verydeep-19.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this ensures the program can use all the gpu resources it can get\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSCOCO dataset http://cocodataset.org/ was downloaded and an hdf5 file was made from 10000 images..\n",
    "#.. out of the 80000 iamges present in the dataset. you'd need to create this dataset\n",
    "hdf5_path = \"../data/image_coco_10000_data.hdf5\"\n",
    "hdf5_file = tables.open_file(hdf5_path, mode='r')\n",
    "content_array = hdf5_file.root.content_images[:,:,:,:]\n",
    "style_array = hdf5_file.root.style_images[:,:,:,:]\n",
    "batch_size = 8\n",
    "style_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weight for the feedfwd network\n",
    "def weight_initializer(weight_input, output_channel_size, filter_size, deconv = False):\n",
    "    \n",
    "    _, rows, columns, input_channel_size = [i.value for i in weight_input.get_shape()]\n",
    "    \n",
    "    if deconv:\n",
    "        weight_shape = [filter_size,filter_size,output_channel_size,input_channel_size]\n",
    "    else:\n",
    "        weight_shape = [filter_size,filter_size,input_channel_size,output_channel_size]\n",
    "\n",
    "    weight_output = tf.Variable(tf.truncated_normal(weight_shape, stddev=0.1, seed=1), dtype=tf.float32)\n",
    "    \n",
    "    return weight_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of activation signals via instance norm fn\n",
    "def instance_norm(net, train=True):\n",
    "    batch, rows, cols, channels = [i.value for i in net.get_shape()]\n",
    "    var_shape = [channels]\n",
    "    mu, sigma_sq = tf.nn.moments(net, [1,2], keep_dims=True)\n",
    "    shift = tf.Variable(tf.zeros(var_shape))\n",
    "    scale = tf.Variable(tf.ones(var_shape))\n",
    "    epsilon = 1e-3\n",
    "    normalized = (net-mu)/(sigma_sq + epsilon)**(.5)\n",
    "    return scale * normalized + shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution block for the feed fwd network\n",
    "def conv_block(block_input, num_filters, filter_size, stride_length, use_relu = True):\n",
    "    \n",
    "    init_weights = weight_initializer(block_input, num_filters, filter_size)\n",
    "    strides = [1,stride_length,stride_length,1]\n",
    "    block_output = tf.nn.conv2d(block_input,init_weights,strides,padding='SAME')\n",
    "    \n",
    "    block_output = instance_norm(block_output)\n",
    "    \n",
    "    if use_relu:\n",
    "        block_output = tf.nn.relu(block_output)\n",
    "    \n",
    "    return block_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse of the convolution process\n",
    "def conv_transpose_block(block_input, num_filters, filter_size, stride_length):\n",
    "    \n",
    "    init_weights = weight_initializer(block_input, num_filters, filter_size, deconv = True)\n",
    "    \n",
    "    batch_size, rows, cols, in_channels = [i.value for i in (block_input).get_shape()]\n",
    "    new_rows, new_cols = int(rows * stride_length), int(cols * stride_length)\n",
    "    batch_size = tf.shape(block_input)[0]\n",
    "    new_shape = [batch_size, new_rows, new_cols, num_filters]\n",
    "    tf_shape = tf.stack(new_shape)\n",
    "    strides = [1,stride_length,stride_length,1]\n",
    "    \n",
    "    block_output = tf.nn.conv2d_transpose(block_input, init_weights, tf_shape, strides, padding='SAME')\n",
    "    \n",
    "    block_output = instance_norm(block_output)\n",
    "    \n",
    "    return tf.nn.relu(block_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual block fn\n",
    "def residual_block(block_input, filter_size = 3, num_filters = 128):\n",
    "    temp = conv_block(block_input, num_filters, filter_size, 1)\n",
    "    return block_input + conv_block(temp, num_filters, filter_size, 1, use_relu = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the input model\n",
    "def fast_model(input_image):\n",
    "    conv_1 = conv_block(input_image, 32, 9, 1)\n",
    "    conv_2 = conv_block(conv_1, 64, 3, 2)\n",
    "    conv_3 = conv_block(conv_2, 128, 3, 2)\n",
    "    \n",
    "    res_1 = residual_block(conv_3, 3, 128) \n",
    "    res_2 = residual_block(res_1, 3, 128)\n",
    "    res_3 = residual_block(res_2, 3, 128)\n",
    "    res_4 = residual_block(res_3, 3, 128)\n",
    "    res_5 = residual_block(res_4, 3, 128)\n",
    "    \n",
    "    conv_t_1 = conv_transpose_block(res_5, 64, 3, 2)\n",
    "    conv_t_2 = conv_transpose_block(conv_t_1, 32, 3, 2)\n",
    "    conv_t_3 = conv_block(conv_t_2, 3, 9, 1, use_relu = False)\n",
    "    final_output = tf.nn.tanh(conv_t_3)*150 + 255.0/2\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate the content cost\n",
    "def content_cost(activation_content, activation_generated):\n",
    "    m, height, width, number_of_channels = activation_generated.get_shape().as_list()\n",
    "    content_loss = 2.0*tf.nn.l2_loss(activation_generated - activation_content)/float(4*height*width*number_of_channels*batch_size)\n",
    "    return content_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function calculates a cost that helps in making the generated image smoother\n",
    "def total_variation(layer):\n",
    "    shape = tf.shape(layer)\n",
    "    height = shape[1]\n",
    "    width = shape[2]\n",
    "    y = tf.slice(layer, [0,0,0,0], tf.stack([-1,height-1,-1,-1])) - tf.slice(layer, [0,1,0,0], [-1,-1,-1,-1])\n",
    "    x = tf.slice(layer, [0,0,0,0], tf.stack([-1,-1,width-1,-1])) - tf.slice(layer, [0,0,1,0], [-1,-1,-1,-1])\n",
    "    tloss = tf.nn.l2_loss(x) / tf.to_float(tf.size(x)) + tf.nn.l2_loss(y) / tf.to_float(tf.size(y))\n",
    "    return tloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the vgg network weights and mean pixel\n",
    "vgg_weights,vgg_mean_pixel = vgg.load_net(vgg_path)\n",
    "\n",
    "#define the vgg network layers which we'll use to calculate style cost. the weights per layer is also mentioned.\n",
    "style_features = {}\n",
    "style_layers = [('conv1_1',0.2),('conv2_1',0.2),('conv3_1',0.2),('conv4_1',0.2),('conv5_1',0.2)]\n",
    "####################################################################################################################\n",
    "#pre-calculate the style cost for the style image. this saves time on sending data back and forth between the RAM and GPU\n",
    "with tf.Graph().as_default(),tf.device('/cpu:0'), tf.Session() as sess:\n",
    "    style_input = tf.placeholder(tf.float32,shape=(1,style_array.shape[1],style_array.shape[2],style_array.shape[3]),name='style_ip')\n",
    "    style_net = vgg.net_preloaded(vgg_weights, (style_input - vgg_mean_pixel), 'max')\n",
    "    \n",
    "    temp_style = style_array[style_num,:,:,:].reshape(1,style_array.shape[1],style_array.shape[2],style_array.shape[3])\n",
    "    \n",
    "    for layer_name, coeff in style_layers:\n",
    "        activation_style = style_net[layer_name].eval(feed_dict={style_input:temp_style})\n",
    "        activation_style = np.reshape(activation_style,(-1,activation_style.shape[3]))\n",
    "        gram_style = np.matmul(activation_style.T, activation_style)/activation_style.size\n",
    "        style_features[layer_name] = gram_style\n",
    "\n",
    "#using tensorflow session as shown below instead of tf.interactivesession makes the training process much faster.\n",
    "# the training time for one epoch/iteration of 10000 imgs went from 720 to 580 secs\n",
    "with tf.Graph().as_default(),tf.Session() as sess:\n",
    "    number_of_content = content_array.shape[0]\n",
    "\n",
    "    content_input = tf.placeholder(tf.float32,shape=(batch_size,content_array.shape[1],content_array.shape[2],content_array.shape[3]),name='content_ip')\n",
    "    ####################################################################################################################\n",
    "    content_net = vgg.net_preloaded(vgg_weights, (content_input - vgg_mean_pixel), 'max')\n",
    "\n",
    "    generated_image = fast_model(content_input/255.0)\n",
    "    generated_net = vgg.net_preloaded(vgg_weights, (generated_image - vgg_mean_pixel), 'max')\n",
    "    ####################################################################################################################\n",
    "    activation_content = content_net['conv4_2']\n",
    "    activation_generated = generated_net['conv4_2']\n",
    "    j_content = content_cost(activation_content, activation_generated)\n",
    "    ###################################################################################################################\n",
    "    style_losses = []\n",
    "\n",
    "    for layer_name, coeff in style_layers:\n",
    "        layer = generated_net[layer_name]\n",
    "        bs, height, width, filters = layer.get_shape().as_list()\n",
    "        size = height * width * filters\n",
    "        feats = tf.reshape(layer, [-1, height * width, filters])\n",
    "        feats_T = tf.transpose(feats, perm=[0,2,1])\n",
    "        grams = tf.matmul(feats_T, feats) / size\n",
    "        style_gram = style_features[layer_name]\n",
    "        style_losses.append(2 * tf.nn.l2_loss(grams - style_gram)/style_gram.size)\n",
    "\n",
    "    j_style = functools.reduce(tf.add, style_losses) / batch_size\n",
    "    ###################################################################################################################\n",
    "    tv_loss = total_variation(generated_image)\n",
    "    ###################################################################################################################\n",
    "    alpha = 7.5\n",
    "    beta = 100.0\n",
    "    gamma = 200.0\n",
    "    total_j = alpha*j_content + beta*j_style + gamma*tv_loss\n",
    "    ###################################################################################################################\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(total_j)\n",
    "    num_iterations = 35 #also called epochs.\n",
    "    sess.run(tf.global_variables_initializer())  \n",
    "    ###################################################################################################################\n",
    "    #change the boolean values below to restore and save model respectively. this is useful when you wanna\n",
    "    #iteratively change the alpha beta gamma or the style layer weights to get the style look that you want \n",
    "    #from a generated image. if youre happy with some weight values and you trained the network for some epochs,  \n",
    "    #you can save and restore the network weights to train for more epochs.\n",
    "    restore_model = False\n",
    "    save_model = True\n",
    "    \n",
    "    #restore variable values. while saving the model further below, im only saving variable values and not the graph. \n",
    "    if(restore_model):\n",
    "        saver =  tf.train.Saver()  \n",
    "        saver.restore(sess,'../models/style/20180819-1/style_model_2')\n",
    "    ###################################################################################################################\n",
    "    num_minibatch = int(number_of_content/batch_size) \n",
    "\n",
    "    for i in range (num_iterations+1):\n",
    "        print(\"iteration number = \", i)\n",
    "        start_time = time.time()    \n",
    "        for j in range(num_minibatch): \n",
    "            temp_content = content_array[j*batch_size:batch_size*(j+1),:,:,:]\n",
    "\n",
    "            _,jc,js,jt,jtv = sess.run([train_step,j_content, j_style,total_j,tv_loss],\n",
    "                                              feed_dict={content_input:temp_content}) \n",
    "\n",
    "        jt = Decimal(np.asscalar(jt))\n",
    "        js = Decimal(np.asscalar(js))\n",
    "        print(\"total cost = \", '{:.9e}'.format(jt))\n",
    "        print(\"content cost = \", jc)\n",
    "        print(\"style cost = \", '{:.9e}'.format(js))\n",
    "        print(\"tv cost = \", jtv)\n",
    "\n",
    "        #this number below selects one of the image from the datasets to apply the transofrmation. this is useful \n",
    "        #when you wanna see how the image keeps changing as the netwokr is getting trained.\n",
    "        imagenum = 4\n",
    "        gen_img = sess.run(generated_image,{content_input:content_array[imagenum:imagenum+batch_size,:,:,:]})\n",
    "        gen_img = gen_img[0].reshape(gen_img.shape[1],gen_img.shape[2],gen_img.shape[3])\n",
    "        gen_img = np.clip(gen_img, 0, 255).astype('uint8')\n",
    "        imshow(gen_img)\n",
    "        #write the generated image to a file to see its progress\n",
    "        imageio.imwrite(\"../data/generated/1-\" + str(i) + \".jpg\", gen_img) \n",
    "\n",
    "        end_time = time.time()\n",
    "        print(\"epoch time is \", (end_time - start_time)) #average epoch time is around 580 secs for 10000 imgs\n",
    "        \n",
    "        #save the model without the graph.\n",
    "        if(save_model and i%5 == 0):\n",
    "            saver_2 = tf.train.Saver()  \n",
    "            saver_2.save(sess,\"../models/style/20180819-1/style_model_2\",write_meta_graph=False)\n",
    "    \n",
    "    \n",
    "    ###################################################################################################################\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
